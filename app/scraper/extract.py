# scraper/extract.py
import logging
import time
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from .utils import format_url
from .storage import (
    get_storage_path,
    save_extracted_text,
)  # ‚úÖ Import storage functions
import re  # ‚úÖ Import regex module

# ‚úÖ Store the first clean extraction
stored_text = None
last_extraction_time = 0
last_url = ""

BASE_DIR = "extracted_data"  # ‚úÖ Base directory where all extracted files will be saved


def extract_text(url):
    global last_extraction_time, last_url

    current_time = time.time()

    # ‚úÖ Prevent skipping different pages (only block same URL within 3 seconds)
    if url == last_url and (current_time - last_extraction_time) < 3:
        logging.warning("‚ö†Ô∏è Duplicate extraction request ignored.")
        return None

    last_url = url
    last_extraction_time = current_time

    logging.info(f"üîç [START] Extracting text from: {url}")
    url = format_url(url)

    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context()
            page = context.new_page()

            page.goto(url, timeout=20000)
            page.wait_for_load_state("networkidle")
            time.sleep(2)

            page_source = page.content()
            if not page_source:
                logging.error("‚ùå No HTML content retrieved.")
                return "Error: No HTML content retrieved."

            # ‚úÖ Print the full HTML source for debugging

            extracted_text = parse_page_text(page_source)

            if "Error:" in extracted_text:
                logging.error("üö® Extraction failed, skipping save.")
                return extracted_text  # Avoid saving errors

            # ‚úÖ Get website folder name & save file
            website_folder, file_name = get_storage_path(url)
            save_extracted_text(extracted_text, website_folder, file_name)

            return extracted_text  # ‚úÖ Return extracted text

        except Exception as e:
            logging.error(f"‚ùå Error extracting text: {e}")
            return f"Error extracting text: {e}"

        finally:
            browser.close()


def parse_page_text(html):
    if not html:
        logging.error("‚ùå Received empty HTML content.")
        return "Error: No HTML content received."

    logging.info("üõ†Ô∏è Parsing page HTML with BeautifulSoup...")
    soup = BeautifulSoup(html, "lxml")

    logging.info("üßπ Removing unnecessary elements...")
    try:
        for tag in soup.find_all(
            [
                "nav",
                "header",
                "footer",
                "aside",
                "script",
                "style",
                "form",
                "button",
                "iframe",
                "noscript",
            ]
        ):
            tag.decompose()
    except Exception as e:
        logging.error(f"‚ùå Error while removing sections: {e}")

    logging.info("üîé Extracting meaningful text elements...")
    try:
        main_content = soup.find("div", id="wsite-content") or soup.find(
            "div", class_="wsite-section-content"
        )

        if not main_content:
            logging.warning("‚ö†Ô∏è No identifiable main content found.")
            return "Error: No meaningful text found on the page."

        text_elements = main_content.find_all(
            ["p", "h1", "h2", "h3", "h4", "h5", "h6", "li", "td", "th", "span", "div"]
        )
        logging.info(f"üìÑ Found {len(text_elements)} text elements in main content.")

        text_list = []
        seen_paragraphs = set()  # ‚úÖ Track paragraphs to remove duplicates
        prev_line = ""  # ‚úÖ Track previous line for funder duplicates

        for el in text_elements:
            extracted_text = el.get_text(separator=" ", strip=True)

            # ‚úÖ Ignore empty or very short text (likely noise)
            if not extracted_text or len(extracted_text) < 5:
                continue

            # ‚úÖ Skip file-related metadata
            if any(
                keyword in extracted_text.lower()
                for keyword in [
                    "file size:",
                    "file type:",
                    "download file",
                    "pdf",
                ]
            ):
                continue

            # ‚úÖ Remove lines that are just a number followed by "kb"
            if re.match(r"^\d+\s*kb$", extracted_text.lower()):
                logging.info(f"üöÄ Skipping file size reference: {extracted_text}")
                continue

            # ‚úÖ Remove duplicate funders
            if extracted_text == prev_line:
                continue
            prev_line = extracted_text

            # ‚úÖ Normalize case & spacing for duplicate detection
            normalized_text = " ".join(extracted_text.split()).lower()

            # ‚úÖ If this text has already been seen, it's a duplicate ‚Üí skip it
            if normalized_text in seen_paragraphs:
                continue

            text_list.append(extracted_text)
            seen_paragraphs.add(normalized_text)

        # ‚úÖ Keep only the second instance with proper line breaks
        if len(text_list) > 1 and text_list[0].startswith(text_list[1][:30]):
            logging.info(
                "üöÄ Removing first duplicated block, keeping formatted version."
            )
            text_list.pop(0)  # Remove first instance (less readable)

        # ‚úÖ Remove last "Contact" word if it's alone at the end
        if text_list and text_list[-1].strip().lower() == "contact":
            logging.info("üöÄ Removing unnecessary 'Contact' at the end.")
            text_list.pop()

        # ‚úÖ Join paragraphs with newlines for readability
        text = "\n\n".join(text_list)

        if not text.strip():
            logging.warning("‚ö†Ô∏è Extracted text is empty.")
            return "Error: Extracted text is empty."

        logging.info(f"‚úÖ Successfully extracted text ({len(text)} characters).")
        return text.strip()

    except Exception as e:
        logging.error(f"‚ùå Error while extracting text: {e}")
        return f"Error: Exception encountered while extracting text: {e}"
